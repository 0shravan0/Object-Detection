{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera = cv2.VideoCapture(0)\n",
    "h,w = None,None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('YOLO-3/YOLO-3-OpenCV/yolo-coco-data/coco.names') as f:\n",
    "    labels = [line.strip() for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = cv2.dnn.readNetFromDarknet('YOLO-3/YOLO-3-OpenCV/yolo-coco-data/yolov3.cfg',\n",
    "                                        'YOLO-3/YOLO-3-OpenCV/yolo-coco-data/yolov3.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_names_all = network.getLayerNames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_name_output = [layers_names_all[i-1]for i in network.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yolo_82', 'yolo_94', 'yolo_106']\n"
     ]
    }
   ],
   "source": [
    "print(layers_name_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "probability_minum = 0.5\n",
    "threshold = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "colours = np.random.randint(0,255,size=(len(labels),3),dtype='uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current frame took0.52963 seconds\n",
      "current frame took0.32934 seconds\n",
      "current frame took0.32984 seconds\n",
      "current frame took0.33676 seconds\n",
      "current frame took0.29363 seconds\n",
      "current frame took0.31049 seconds\n",
      "current frame took0.31893 seconds\n",
      "current frame took0.31546 seconds\n",
      "current frame took0.30454 seconds\n",
      "current frame took0.30454 seconds\n",
      "current frame took0.44442 seconds\n",
      "current frame took0.32141 seconds\n",
      "current frame took0.33133 seconds\n",
      "current frame took0.50790 seconds\n",
      "current frame took0.35762 seconds\n",
      "current frame took0.35265 seconds\n",
      "current frame took0.41014 seconds\n",
      "current frame took0.36010 seconds\n",
      "current frame took0.42161 seconds\n",
      "current frame took0.65671 seconds\n",
      "current frame took0.48760 seconds\n",
      "current frame took0.46426 seconds\n",
      "current frame took0.54858 seconds\n",
      "current frame took0.42309 seconds\n",
      "current frame took0.37115 seconds\n",
      "current frame took0.47517 seconds\n",
      "current frame took0.32934 seconds\n",
      "current frame took0.31865 seconds\n",
      "current frame took0.43301 seconds\n",
      "current frame took0.32786 seconds\n",
      "current frame took0.61802 seconds\n",
      "current frame took0.41069 seconds\n",
      "current frame took0.33480 seconds\n",
      "current frame took0.36182 seconds\n",
      "current frame took0.35563 seconds\n",
      "current frame took0.30303 seconds\n",
      "current frame took0.30355 seconds\n",
      "current frame took0.27827 seconds\n",
      "current frame took0.28371 seconds\n",
      "current frame took0.27975 seconds\n",
      "current frame took0.26829 seconds\n",
      "current frame took0.26536 seconds\n",
      "current frame took0.28220 seconds\n",
      "current frame took0.29859 seconds\n",
      "current frame took0.28021 seconds\n",
      "current frame took0.28371 seconds\n",
      "current frame took0.27923 seconds\n",
      "current frame took0.27429 seconds\n",
      "current frame took0.27677 seconds\n",
      "current frame took0.26536 seconds\n",
      "current frame took0.27627 seconds\n",
      "current frame took0.29462 seconds\n",
      "current frame took0.27825 seconds\n",
      "current frame took0.26735 seconds\n",
      "current frame took0.28222 seconds\n",
      "current frame took0.26883 seconds\n",
      "current frame took0.27429 seconds\n",
      "current frame took0.26685 seconds\n",
      "current frame took0.26090 seconds\n",
      "current frame took0.27230 seconds\n",
      "current frame took0.27280 seconds\n",
      "current frame took0.27674 seconds\n",
      "current frame took0.29067 seconds\n",
      "current frame took0.29710 seconds\n",
      "current frame took0.27076 seconds\n",
      "current frame took0.27674 seconds\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    _,frame = camera.read()\n",
    "\n",
    "    if w is None or h is None:\n",
    "        h,w = frame.shape[:2]\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(frame,1/255.0,(416,416),swapRB=True,crop= False)\n",
    "\n",
    "    #implementing forward pass\n",
    "    network.setInput(blob)\n",
    "    start = time.time()\n",
    "    outpur_from_network = network.forward(layers_name_output)\n",
    "    end = time.time()\n",
    "\n",
    "    print('current frame took{:.5f} seconds'.format(end-start))\n",
    "\n",
    "    bounding_boxes=[]\n",
    "    confidences =[]\n",
    "    class_numbers = []\n",
    "\n",
    "    for results in outpur_from_network:\n",
    "        for detected_objects in results:\n",
    "\n",
    "            scores = detected_objects[5:]\n",
    "\n",
    "            class_current = np.argmax(scores)\n",
    "\n",
    "            confidence_current = scores[class_current]\n",
    "\n",
    "\n",
    "            if confidence_current>probability_minum:\n",
    "\n",
    "                box_current = detected_objects[0:4] * np.array([w, h, w, h])\n",
    "\n",
    "                # Now, from YOLO data format, we can get top left corner coordinates\n",
    "                # that are x_min and y_min\n",
    "                x_center, y_center, box_width, box_height = box_current\n",
    "                x_min = int(x_center - (box_width / 2))\n",
    "                y_min = int(y_center - (box_height / 2))\n",
    "\n",
    "                # Adding results into prepared lists\n",
    "                bounding_boxes.append([x_min, y_min,\n",
    "                                       int(box_width), int(box_height)])\n",
    "                confidences.append(float(confidence_current))\n",
    "                class_numbers.append(class_current)\n",
    "\n",
    "\n",
    "    results = cv2.dnn.NMSBoxes(bounding_boxes, confidences,\n",
    "                                        probability_minum, threshold)\n",
    "    \n",
    "\n",
    "\n",
    "    if len(results) > 0:\n",
    "        # Going through indexes of results\n",
    "        for i in results.flatten():\n",
    "            # Getting current bounding box coordinates,\n",
    "            # its width and height\n",
    "            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n",
    "            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n",
    "\n",
    "            # Preparing colour for current bounding box\n",
    "            # and converting from numpy array to list\n",
    "            colour_box_current = colours[class_numbers[i]].tolist()\n",
    "\n",
    "            # # # Check point\n",
    "            # print(type(colour_box_current))  # <class 'list'>\n",
    "            # print(colour_box_current)  # [172 , 10, 127]\n",
    "\n",
    "            # Drawing bounding box on the original current frame\n",
    "            cv2.rectangle(frame, (x_min, y_min),\n",
    "                          (x_min + box_width, y_min + box_height),\n",
    "                          colour_box_current, 2)\n",
    "\n",
    "            # Preparing text with label and confidence for current bounding box\n",
    "            text_box_current = '{}: {:.4f}'.format(labels[int(class_numbers[i])],\n",
    "                                                   confidences[i])\n",
    "\n",
    "            # Putting text with label and confidence on the original image\n",
    "            cv2.putText(frame, text_box_current, (x_min, y_min - 5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, colour_box_current, 2)\n",
    "            \n",
    "    \n",
    "\n",
    "    cv2.namedWindow('YOLO v3 Real Time Detections', cv2.WINDOW_NORMAL)\n",
    "    # Pay attention! 'cv2.imshow' takes images in BGR format\n",
    "    cv2.imshow('YOLO v3 Real Time Detections', frame)\n",
    "\n",
    "    # Breaking the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Releasing camera\n",
    "camera.release()\n",
    "# Destroying all opened OpenCV windows\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "         \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee624a4eec235785d3aab9efc94faaa6a1395aba77386655f1fb0ab8037a5921"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
